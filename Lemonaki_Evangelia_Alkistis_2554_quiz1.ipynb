{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# student name and AM:\n",
    "\n",
    "Lemonaki Evangelia-Alkistis, 2554\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1 - Part I - Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Describe the linear regression problem in symbolic form (using markdown and latex) assuming the data are subject to errors. What are the basic assumptions made? What is the form of **likelihood** function for this case? \n",
    "\\end{exercise}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "\n",
    "In linear regression we have a function $$f(i) = \\beta_0 + \\beta_1*x_{i1} + ... + \\beta_p*x_{ip} + \\epsilon_i$$ and the parameters are the $\\beta_i, i=0...p$ and $\\epsilon_i$ is the error. We use ordinary least squares approach given a set of $(y_i,x_i)$ pairs. \n",
    "\n",
    "Data Assumption: $$\\mathcal D = \\{(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n)\\}\\ y\\_{i} \\\\in \\\\mathbb{R}\\$$ and we calculate the weight vector $ w = [\\beta_0, \\beta_1,...,\\beta_p]$ with $\\beta_0 = 0$. \n",
    "\n",
    "Model Assumption:\n",
    "\n",
    "* Each response generated by a linear model plus some Gaussian noise\n",
    "\\$y\\_{i} = \\\\mathbf{w}\\^\\\\top\\\\mathbf{x}\\_i +\n",
    "\\\\epsilon\\_i\\$ \n",
    "\n",
    "* Noise $\\epsilon$ is drawn from a Gaussian distribution\n",
    "\n",
    "\\$\\$\\\\epsilon\\_i \\\\sim N(0, \\\\sigma\\^2)\\$\\$\n",
    "\n",
    "* Each response $y$ then becomes a draw from the following Gaussian:\n",
    "\n",
    "\\$\\$\\\\Rightarrow y\\_i|\\\\mathbf{x}\\_i \\\\sim\n",
    "N(\\\\mathbf{w}\\^\\\\top\\\\mathbf{x}\\_i, \\\\sigma\\^2) \\$\\$\n",
    "\n",
    "Likelihood function:\n",
    "\n",
    "\\$\\$\\\\Rightarrow\n",
    "P(y\\_i|\\\\mathbf{x}\\_i,\\\\mathbf{w})=\\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma\\^2}}e\\^{-\\\\frac{(\\\\mathbf{x}\\_i\\^\\\\top\\\\mathbf{w}-y\\_i)\\^2}{2\\\\sigma\\^2}}\\$\\$ and $\\sigma$ is unknown, so it must be estimated.\n",
    "\n",
    "In words, we assume that the data is drawn from a \"line\"  ùê∞‚ä§ùê±  through the origin (one can always add a bias / offset through an additional dimension). For each data point with features  ùê±ùëñ , the label  ùë¶  is drawn from a Gaussian with mean  ùê∞‚ä§ùê±ùëñ  and variance  ùúé2 . Our task is to estimate the weights (it is called slope in 1D case)  ùê∞  from the data.\n",
    "\n",
    "assume a fixed unknown mean and variance $\\sigma^{2}$ corresponding to $y=\\mu+\\varepsilon$ regression model (here $\\varepsilon$ is a zero mean and $\\sigma^{2}$ variance):\n",
    "$$\n",
    "y \\sim N\\left(\\mu, \\sigma^{2}\\right)\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}(\\mu \\mid y)=\\prod_{i=1}^{n} P_{Y}\\left(y_{i} \\mid \\mu, \\sigma^{2}\\right)=\\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mu}=\\arg \\max _{\\mu} \\mathcal{L}(\\mu \\mid y) &=\\arg \\max _{\\mu} \\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(y_{i}-\\mu\\right)}{2 \\sigma^{2}}} \\\\\n",
    "&=\\arg \\min _{\\mu} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mu}=\\arg \\max _{\\mu} \\mathcal{L}(\\mu \\mid y) =\\arg \\max _{\\mu} \\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(y_{i}-\\mu\\right)}{2 \\sigma^{2}}}\\\\ \n",
    "=\\arg \\max _{\\mu} \\log \\left(\\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}}\\right)\\\\\n",
    "=\\arg \\max _{\\mu} \\sum_{i=1}^{n} \\log \\left(\\frac{1}{\\sigma \\sqrt{2 \\pi}}\\right)+\\log \\left(e^{-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}}\\right) \\\\\n",
    "=\\arg \\max _{\\mu} \\sum_{i=1}^{n} \\log \\left(e^{-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}}\\right)  \n",
    "=\\arg \\min _{\\mu} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial \\mu} \\log \\mathcal{L}(\\mu \\mid y)=0 \\\\\n",
    "\\frac{\\partial}{\\partial \\mu} \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2}=0 \\\\\n",
    "\\sum_{i=1}^{n}-2\\left(y_{i}-\\mu\\right)=0 \\\\\n",
    "n \\mu=\\sum_{i=1}^{n} y_{i} \\\\\n",
    "\\mu=\\frac{1}{n} \\sum_{i=1}^{n} y_{i}\n",
    "\\end{array}\n",
    "\n",
    "Finally, the expected value of $y$ is just the expected value of a normal distribution, which is just equal its mean:\n",
    "$$\n",
    "E(y)=\\mu\n",
    "$$\n",
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Assuming that $X$ is design matrix (given values for independent variables or features) and $y$ the vector values of the dependent variable or response, and the linear approximation of data is $y = Xw$ where the $w$ is the vector of unknown coefficients of the regression approximate function:\n",
    "\n",
    "(i) formulate symbolically the optimization problem we must solve to find the OLS (ordinary least squares solution) \n",
    "\n",
    "(ii) formulate symbolically the optimization problem we must solve to find the Lasso approximation to the data. Explain its usefulness.\n",
    "\n",
    "(iii)formulate symbolically the optimization problem we must solve to find the Ridge  approximation to the data. Explain its usefulness.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "\n",
    "(i) $$\n",
    "\\mathbf{W}=\\underset{\\mathbf{w}}{\\operatorname{argmax}} P\\left(y_{1}, \\mathbf{x}_{1}, \\ldots, y_{n}, \\mathbf{x}_{n} \\mid \\mathbf{w}\\right) \\\\\n",
    "=\\underset{\\mathbf{w}}{\\operatorname{argmin}} \\frac{1}{n} \\sum_{i=1}^n\\left(\\mathbf{x}_{i}^{\\top} \\mathbf{w}-y_{i}\\right)^{2}\n",
    "$$\n",
    "We are minimizing a *loss function*, \\$l(\\\\mathbf{w}) =\n",
    "\\\\frac{1}{n}\\\\sum\\_{i=1}\\^n\n",
    "(\\\\mathbf{x}\\_i\\^\\\\top\\\\mathbf{w}-y\\_i)\\^2\\$. This loss function is called OLS.\n",
    "-   Squared loss.\n",
    "-   No regularization.\n",
    "-   Closed form: \\$\\mathbf{w} = (\\mathbf{X X\\^\\top})\\^{-1}\\mathbf{X}\n",
    "    \\mathbf{y}\\^\\top\\$.\n",
    "    \n",
    "(ii)\n",
    "-   \\$\\operatorname\\*{min}\\_{\\mathbf{\\mathbf{w}}}\n",
    "    \\frac{1}{n}\\sum\\_{i=1}\\^n\n",
    "    (\\mathbf{x}\\_i\\^\\top\\mathbf{w}-y\\_i)\\^2 + \\lambda\n",
    "    |\\mathbf{w}|$.\n",
    "\n",
    "(iii) \n",
    "-   \\$\\operatorname\\*{min}\\_{\\mathbf{\\mathbf{w}}}\n",
    "    \\frac{1}{n}\\sum\\_{i=1}\\^n\n",
    "    (\\mathbf{x}\\_i\\^\\top\\mathbf{w}-y\\_i)\\^2 + \\lambda\n",
    "    ||\\mathbf{w}||\\_2\\^2\\$.\n",
    "-   Squared loss.\n",
    "-   \\$l2\\text{-regularization}\\$.\n",
    "-   Closed form: \\$\\mathbf{w} = (\\mathbf{X X\\^{\\top}}+\\lambda\n",
    "    \\mathbf{I})\\^{-1}\\mathbf{X} \\mathbf{y}\\^\\top\\$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Apply the ridge regression (choose $\\lambda = 0.01$) to the `Advertising.csv` dataset using  80% of the given data to train the model and predict the value of the response variable on the rest (20%) of data points. Measure the accuracy.  You can use the sklearn library.  \n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TV  Radio  Newspaper  Sales\n",
      "0  230.1   37.8       69.2   22.1\n",
      "1   44.5   39.3       45.1   10.4\n",
      "2   17.2   45.9       69.3    9.3\n",
      "(200, 4)\n",
      "1.413400112785369\n",
      "Score: 0.8924475589245054\n",
      "[('TV', 0.04639297397141479), ('Radio', 0.17658368246831718), ('Newspaper', 0.003277002581042999)]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = read_csv('advertising.csv')\n",
    "print(dataset.head(3))\n",
    "print(dataset.shape)\n",
    "features = ['TV', 'Radio', 'Newspaper']\n",
    "X = dataset[features]\n",
    "y = dataset.Sales\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "#alpha is the lamba\n",
    "ridgereg = Ridge(alpha=0.01, normalize=True)\n",
    "ridgereg.fit(X_train, y_train)\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Score:\", ridgereg.score(X_test,y_test))\n",
    "print(list (zip(features, ridgereg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "\n",
    "(i) Describe  symbolically the solution of the regression problem using MLE (Maximum Likelihood Estimation) by specifying the associated optimization problem that you must solve. Apply this technique to solve numerically the above problem (`Advertizing.csv`) and describe the output you have obtain. You can use the sklearn library.\n",
    "\n",
    "(ii) What is the main difference between the MLE and OLS methods? Notice that in sklearn the above method is referred as **gaussian_process**.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "(i) MLE is a technique used for estimating the parameters of a given s=distribution, using some observed data.MLE does that by finding particular values for the parameters (mean and variance) so that the resultant model with those parameters (mean and variance) would have generated the data. This is equivalent of solving the following optimization problem: $$\n",
    "\\mathbf{W}=\\underset{\\mathbf{w}}{\\operatorname{argmax}} P\\left(y_{1}, \\mathbf{x}_{1}, \\ldots, y_{n}, \\mathbf{x}_{n} \\mid \\mathbf{w}\\right) \\\\\n",
    "=\\underset{\\mathbf{w}}{\\operatorname{argmin}} \\frac{1}{n} \\sum_{i=1}\\left(\\mathbf{x}_{i}^{\\top} \\mathbf{w}-y_{i}\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TV  Radio  Newspaper  Sales\n",
      "0  230.1   37.8       69.2   22.1\n",
      "1   44.5   39.3       45.1   10.4\n",
      "2   17.2   45.9       69.3    9.3\n",
      "1.4123126737861804\n",
      "Score: 0.8926129922308434\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GaussianProcess\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "dataset=pd.read_csv('advertising.csv')\n",
    "X = dataset[features]\n",
    "y = dataset.Sales\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "print(dataset.head(3))\n",
    "gauss = GaussianProcess(kernel=kernel, random_state=0).fit(X_train, y_train)\n",
    "y_pred = gauss.predict(X_test)\n",
    "print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Score:\", gauss.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) The ordinary least squares, or OLS, can also be called the linear least squares. This is a method for approximately determining the unknown parameters located in a linear regression model. It minimizes the square of the residuals.\n",
    "\n",
    "Maximum likelihood estimation, or MLE, is a method used in estimating the parameters of a statistical model and for fitting a statistical model to data. It maximizes the probability of observing the dataset given a model and its parameters.\n",
    "\n",
    "OLS: no assumption on the random error term of the linear model. \n",
    "\n",
    "MLE: random error is assumed to follow certain distribution (usually follow a normal distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1 - Part II - Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "\n",
    "(i) In the binary classification problem assume that  the dataset for the independent variables is the matrix $X$ with dimensions $n \\times p$  where $n$ indicates the number of data and $p$ the number of independent variables. The method tries to identify a boundary that separates the data points in two clusters. Indicate symbolically, in vector form, the equation of a linear boundary that separates the two clusters and specify the optimization problem that will determine this boundary.\n",
    "\n",
    "(ii) Describe the logistic regression method symbolically utilizing the odds mathematical concept. \n",
    "\n",
    "(iii) Apply this method on the iris dataset (the design matrix or  values of independent variables). You can import the iris dataset using the following python statement:\n",
    "\n",
    "        from sklearn.datasets import load_iris\n",
    "        data = load_iris()\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "(i) We have a decision boundary that separates the two classes. It is a line with type $y = b_0 + b_1*x_1+...$ $$ H_{\\omega}(x) = g(\\omega^T*x), g(z) = \\frac{1}{1+e^{-z}} $$ So $$ h_{\\omega}(x) = g(\\omega + \\omega*x_1 + \\omega*x_2), \\omega = [\\omega_0 \\omega_1 \\omega_2]$$ \n",
    "\n",
    "Suppose that if $y=1$ and $\\omega = [-3 ,1, 1]$ we have $ -3 + x_1 + x_2 \\geq 0$ and $ x_1+x_2=3$ is the decision boundary.\n",
    "\n",
    "(ii) When performing logistic regression we use $\\pi = Pr(y=1|x) = \\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}$, probability of y=1, given x. The logistic function takes on an ‚ÄúS‚Äù shape, where y is bounded by $[0,1]$. In order to interpret the outputs of a logistic function we must understand the difference between probability and odds. $$Odds = \\frac{\\pi}{1-\\pi}$$ The odds of a logistic function is $\\frac{\\pi}{1-\\pi} = \\frac{\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}}{\\frac{1-e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}} = e^{\\beta_0+\\beta_1x}$\n",
    "\n",
    "If we take the logarithm of the odds, we get a linear equation $loag(\\frac{\\pi}{1-\\pi}) = log(e^{\\beta_0+\\beta_1x}) = \\beta_0+\\beta_1x$\n",
    "\n",
    "In logistc regression $\\beta_1$ represents the change in the log-odds for a unit change in x. So, e^{\\beta_1} gives the change in the odds for a unit in x.\n",
    "\n",
    "(iii) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16222142113076254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "iris_data = load_iris()\n",
    "#print(iris_data.DESCR)\n",
    "X, y = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names), pd.DataFrame(data=iris_data.target, columns=[\"iris_type\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train, y_test = np.ravel(y_train), np.ravel(y_test)\n",
    "logreg = LogisticRegression(random_state=0, solver='lbfgs', multi_class=\"multinomial\", max_iter=400)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1 - Part III - PCA analysis of a toy problem\n",
    "\n",
    "\\begin{exercise}\n",
    "\n",
    "Apply manually all the steps of PCA analysis in the following toy example\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input data\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[10,20,10],\n",
    "\n",
    "              [2,5,2],\n",
    "\n",
    "              [8,17,7],\n",
    "\n",
    "              [9,20,10],\n",
    "\n",
    "              [12,22,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 20 10]\n",
      " [ 2  5  2]\n",
      " [ 8 17  7]\n",
      " [ 9 20 10]\n",
      " [12 22 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 8.2, 16.8,  8. ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the data\n",
    "print(X)\n",
    "# specify the data as a matrix\n",
    "X = np.matrix(X)\n",
    "# compute the mean value of column of X\n",
    "meanVals = np.mean(X, axis=0)\n",
    "meanVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.2  25.3  13.5 ]\n",
      " [25.3  46.7  24.75]\n",
      " [13.5  24.75 13.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# Produce a zero-mean (centerd) version of X name it A\n",
    "# A is the zero-mean (centered) version of X\n",
    "# Produce the covariance matrix of X and print it\n",
    "# C is the covarianvce matrix of X\n",
    "# print (C)\n",
    "# Note that the covariance C = (1/(N-1)) A.T*A\n",
    "A = X - meanVals             # MC is the zero-mean (centered) version of X\n",
    "CovM = np.cov(A, rowvar=0)    # CovM is the covarianvce matrix of M\n",
    "print (np.dot(A.T,A)/(np.shape(X)[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [73.72  0.38  0.3 ]\n",
      "Eigenvectors: [[ 0.43  0.9  -0.04]\n",
      " [ 0.79 -0.41 -0.45]\n",
      " [ 0.42 -0.16  0.89]]\n"
     ]
    }
   ],
   "source": [
    "# Produce the eigenvalues and eigenvectors of the covariance matrix:\n",
    "from numpy.linalg import eig\n",
    "np.set_printoptions(precision=2,suppress=True)\n",
    "# missing code\n",
    "e, ev = eig(CovM)\n",
    "print (\"Eigenvalues:\",e)\n",
    "print (\"Eigenvectors:\",ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  ]\n",
      " [ 0.17]\n",
      " [-0.1 ]\n",
      " [-0.9 ]\n",
      " [ 0.83]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the full data into the new feature space based on the eigenvectors:\n",
    "# missing code\n",
    "newFeatures = ev[:,1].T\n",
    "XTrans = np.dot(newFeatures, A.T)\n",
    "print (XTrans.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.17]\n",
      " [-14.61]\n",
      " [ -0.35]\n",
      " [  3.74]\n",
      " [  7.05]]\n"
     ]
    }
   ],
   "source": [
    "# Typically, we want a lower-dimensional space. We can sort the eigenvectors in the decreasing order of \n",
    "# their eigenvalues and take the top k. We'll take only the top first principal component \n",
    "#(since it has the largest eigenvalue, no sorting necessary):\n",
    "\n",
    "reducedFeatures = ev[:,0].T\n",
    "\n",
    "redcuedXTrans = np.dot(reducedFeatures, A.T)\n",
    "\n",
    "print (redcuedXTrans.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.17]\n",
      " [14.61]\n",
      " [ 0.35]\n",
      " [-3.74]\n",
      " [-7.05]]\n",
      "[-4.17 14.61  0.35 -3.74 -7.05]\n"
     ]
    }
   ],
   "source": [
    "# Use Scikit-learn decomposition module to do the same thing:\n",
    "from sklearn import decomposition\n",
    "\n",
    "# missing code\n",
    "pca2 = decomposition.PCA(n_components=1)\n",
    "XTrans2 = pca2.fit_transform(X)\n",
    "\n",
    "print (XTrans2)\n",
    "\n",
    "redcuedXTrans = XTrans2[:,0]\n",
    "\n",
    "print (redcuedXTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{example}\n",
    "An example of PCA analysis using sklearn library\n",
    "\\end{example}\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Mean Matrix:\n",
      " [[ 0.69  0.49]\n",
      " [-1.31 -1.21]\n",
      " [ 0.39  0.99]\n",
      " [ 0.09  0.29]\n",
      " [ 1.29  1.09]\n",
      " [ 0.49  0.79]\n",
      " [ 0.19 -0.31]\n",
      " [-0.81 -0.81]\n",
      " [-0.31 -0.31]\n",
      " [-0.71 -1.01]] \n",
      "\n",
      "Covariance Matrix:\n",
      " [[0.61655556 0.61544444]\n",
      " [0.61544444 0.71655556]] \n",
      "\n",
      "Eigenvalues:\n",
      " [0.0490834  1.28402771] \n",
      "\n",
      "Eigenvectors:\n",
      " [[-0.73517866 -0.6778734 ]\n",
      " [ 0.6778734  -0.73517866]] \n",
      "\n",
      "[[-0.82797019]\n",
      " [ 1.77758033]\n",
      " [-0.99219749]\n",
      " [-0.27421042]\n",
      " [-1.67580142]\n",
      " [-0.9129491 ]\n",
      " [ 0.09910944]\n",
      " [ 1.14457216]\n",
      " [ 0.43804614]\n",
      " [ 1.22382056]]\n",
      "[[-0.82797019]\n",
      " [ 1.77758033]\n",
      " [-0.99219749]\n",
      " [-0.27421042]\n",
      " [-1.67580142]\n",
      " [-0.9129491 ]\n",
      " [ 0.09910944]\n",
      " [ 1.14457216]\n",
      " [ 0.43804614]\n",
      " [ 1.22382056]]\n"
     ]
    }
   ],
   "source": [
    "#An example of using PCA for dimensionality reduction:\n",
    "from numpy.linalg import eig\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "M = np.array([[2.5, 2.4],[0.5, 0.7], [2.2, 2.9],[1.9, 2.2],[3.1, 3.0],\n",
    "[2.3, 2.7],[2, 1.6], [1, 1.1],[1.5, 1.6],[1.1, 0.9]])\n",
    "meanM = np.mean(M, axis=0)\n",
    "MC = M - meanM             # MC is the zero-mean (centered) version of X\n",
    "CovM = np.cov(MC, rowvar=0)    # CovM is the covarianvce matrix of M\n",
    "print (\"Zero Mean Matrix:\\n\", MC,\"\\n\")\n",
    "print (\"Covariance Matrix:\\n\", CovM,\"\\n\")\n",
    "eigVals, eigVecs = eig(CovM)\n",
    "print (\"Eigenvalues:\\n\", eigVals,\"\\n\")\n",
    "print (\"Eigenvectors:\\n\", eigVecs,\"\\n\")\n",
    "newFeatures = eigVecs[:,1].T\n",
    "MTrans = np.dot(newFeatures, MC.T)\n",
    "print (np.mat(MTrans).T)\n",
    "pca2 = PCA(n_components=1)\n",
    "MTrans2 = pca2.fit_transform(M)\n",
    "print (MTrans2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Regression - Theory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Linear Regression\n",
    "---\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "![](linreg.png) \n",
    "\n",
    "**Data Assumption**:\n",
    "\n",
    "\\$\\mathcal D = \\{(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n)\\}\\$\n",
    "\\$y\\_{i} \\\\in \\\\mathbb{R}\\$\n",
    "\n",
    "**Output**\n",
    "\n",
    "We want to estimate the weight vector $\\mathbf w$ of the linear regression model \\$y\\_{i} = \\\\mathbf{w}\\^\\\\top\\\\mathbf{x}\\_i +\n",
    "\\\\epsilon\\_i\\$ \n",
    "\n",
    "**Model Assumption**: \n",
    "\n",
    "* Each response generated by a linear model plus some Gaussian noise\n",
    "\\$y\\_{i} = \\\\mathbf{w}\\^\\\\top\\\\mathbf{x}\\_i +\n",
    "\\\\epsilon\\_i\\$ \n",
    "\n",
    "* Noise $\\epsilon$ is drawn from a Gaussian distribution\n",
    "\n",
    "\\$\\$\\\\epsilon\\_i \\\\sim N(0, \\\\sigma\\^2)\\$\\$\n",
    "\n",
    "* Each response y then becomes a draw from the following Gaussian:\n",
    "\n",
    "\\$\\$\\\\Rightarrow y\\_i|\\\\mathbf{x}\\_i \\\\sim\n",
    "N(\\\\mathbf{w}\\^\\\\top\\\\mathbf{x}\\_i, \\\\sigma\\^2) \\$\\$\n",
    "\n",
    "* Probability of each response variable is\n",
    "\n",
    "\\$\\$\\\\Rightarrow\n",
    "P(y\\_i|\\\\mathbf{x}\\_i,\\\\mathbf{w})=\\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma\\^2}}e\\^{-\\\\frac{(\\\\mathbf{x}\\_i\\^\\\\top\\\\mathbf{w}-y\\_i)\\^2}{2\\\\sigma\\^2}}\\$\\$ \n",
    "and it is called the **likelihood function**.\n",
    "\n",
    "In words, we assume that the data is drawn from a \"line\"\n",
    "\\$\\\\mathbf{w}\\^\\\\top \\\\mathbf{x}\\$ through the origin (one can always\n",
    "add a bias / offset through an additional dimension). For each data point with features\n",
    "\\$\\\\mathbf{x}\\_i\\$, the label \\$y\\$ is drawn from a Gaussian with mean\n",
    "\\$\\\\mathbf{w}\\^\\\\top \\\\mathbf{x}\\_i\\$ and variance \\$\\\\sigma\\^2\\$. Our\n",
    "task is to estimate the slope \\$\\\\mathbf{w}\\$ from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating with MLE (Maximum Likelihood Estimation) \n",
    "\n",
    "Maximum likelihood estimation (MLE) is a technique used for estimating the parameters of a given distribution, using some observed data. For example, if a population is known to follow a ‚Äúnormal distribution‚Äù but the ‚Äúmean‚Äù and ‚Äúvariance‚Äù are unknown, MLE can be used to estimate them using a limited sample of the population. MLE does that by finding particular values for the parameters (mean and variance) so that the resultant model with those parameters (mean and variance) would have generated the data. This is equivalent of solving the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned} \\mathbf{w} &= \n",
    "\\underset {\\mathbf{w}} {argmax}\n",
    "P(y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n|\\mathbf{w})\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax} \\prod_{i=1}^n\n",
    "P(y_i,\\mathbf{x}_i|\\mathbf{w}) & \\textrm{(Because of\n",
    "independence.)}\n",
    "\\\\ &= \n",
    "\\underset {\\mathbf{w}} {argmax} \\prod_{i=1}^n\n",
    "P(y_i|\\mathbf{x}_i,\\mathbf{w})P(\\mathbf{x}_i|\\mathbf{w}) &\n",
    "\\textrm{(Chain rule of probability)}\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax} \\prod_{i=1}^n\n",
    "P(y_i|\\mathbf{x}_i,\\mathbf{w})P(\\mathbf{x}_i) &\n",
    "\\textrm{ ($\\mathbf{x_i}$ is independent of $\\mathbf{w_i}$)}\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax} \\prod_{i=1}^n\n",
    "P(y_i|\\mathbf{x}_i,\\mathbf{w}) & \n",
    "\\textrm{($P(\\mathbf{x}_i)$ is\n",
    "a constant - can be dropped.)}\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax} \\sum_{i=1}^n\n",
    "\\log [P(y_i|\\mathbf{x}_i,\\mathbf{w})] & \n",
    "\\textrm{log is a monotonic function}\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax} \\sum_{i=1}^n\n",
    "[ \\log(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}) +\n",
    "\\log (e^{-\\frac{(\\mathbf{x}_i^\\top \\mathbf{w}-y_i)^2}{2\\sigma^2}})]\n",
    "& \\textrm{Plugging in probability distribution.}\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax}\n",
    "-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\n",
    "(\\mathbf{x}_i^\\top \\mathbf{w}-y_i)^2 & \n",
    "\\textrm{First term is a\n",
    "constant, and $\\log(e^z)=z$}\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmin}\n",
    "\\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2 &\n",
    "\\textrm{Always minimize; $\\frac{1}{n}$ makes the loss interpretable\n",
    "(average squared error).}\n",
    "\\end{aligned} \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are minimizing a *loss function*, \\$l(\\\\mathbf{w}) =\n",
    "\\\\frac{1}{n}\\\\sum\\_{i=1}\\^n\n",
    "(\\\\mathbf{x}\\_i\\^\\\\top\\\\mathbf{w}-y\\_i)\\^2\\$. This particular loss\n",
    "function is also known as the squared loss or Ordinary Least Squares\n",
    "(OLS). OLS can be optimized with gradient descent, Newton's method, or\n",
    "in closed form.\n",
    "\n",
    "**Closed Form:** \n",
    "$\\mathbf{w} = (\\mathbf{X}\n",
    "\\mathbf{X}^T)^{-1}\\mathbf{X}\\mathbf{y}^T$ where\n",
    "$\\mathbf{X}=[\\mathbf{x}_1,\\dots,\\mathbf{x}_n]$\n",
    "and $\\mathbf{y}=[y_1,\\dots,y_n]$.\n",
    "\n",
    "\\begin{remark}\n",
    "Least Square Estimate is same as Maximum Likelihood Estimate under a Gaussian model ! \n",
    "\n",
    "\\end{remark}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating with MAP (Maximum-a-Posteriori Estimation)\n",
    "\n",
    "**What is MAP?**\n",
    "\n",
    "The MAP criterion is derived from Bayes Rule, i.e.\n",
    "\\begin{equation}\n",
    "\tP(A \\vert B) = \\frac{P(B \\vert A)P(A)}{P(B)} \n",
    "\\end{equation}\n",
    "\n",
    "If $B$ is chosen to be your data $\\mathcal D$ and A is chosen to be the parameters that you'd want to estimate, call it $w$, you will get\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\underbrace{P(w \\vert \\mathcal{D})}_{\\text{Posterior}} = \n",
    "\t\\frac{1}{\\underbrace{P(\\mathcal{D})}_{\\text{Normalization}}}\n",
    "\\overbrace{P(\\mathcal{D} \\vert w)}^{\\text{Likelihood}}\\overbrace{P(w)}^{\\text{Prior}}  \\tag{0}\n",
    "\\end{equation}\n",
    "\n",
    "In the case of regression \n",
    "\n",
    "\\begin{equation}\n",
    "\tw \\sim \\mathcal{N}(0, \\lambda^{-1}I)\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "\tD \\vert w \\sim \\mathcal{N}(w^T x, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "## Deriving the Gaussian Prior \n",
    "\n",
    "Using the Normal distribution PDF with mean vector $\\mu$ and covariance matrix $\\Sigma$, which in the Multivariate case is\n",
    "\n",
    "\\begin{equation}\n",
    "\tf(x) = \\frac{1}{\\sqrt{(2\\pi)^{N} \\det \\Sigma}}exp(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)) \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "$w$  is a Normal with zero mean $\\mu=0$ and variance $\\Sigma=\\lambda^1I$. Plug it in the above equation, you will get\n",
    "\n",
    "\\begin{equation}\n",
    "\tf(w) =  \\frac{1}{\\sqrt{(2\\pi)^N \\frac{1}{\\lambda^D}}}exp(-\\frac{1}{2}(w - 0)^T (\\frac{1}{\\lambda} I)^{-1} (w - 0))\n",
    "\\end{equation}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{equation}\n",
    "\tf(w) = \\frac{\\lambda^{\\frac{N}{2}}}{(2\\pi)^{\\frac{N}{2}}}exp(-\\frac{\\lambda}{2} w^T w)\n",
    "\\end{equation}\n",
    "\n",
    "**Deriving the Likelihood function**\n",
    "\n",
    "From the above equation we get $f(\\mathcal D|w)$ finding $f(y_k|w)$ with mean $w^\\top x$ and variance $\\sigma^2$, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "\tf(y_k \\vert w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y_k- x^Tw)^2)\n",
    "\\end{equation}\n",
    "\n",
    "But since $y_1 \\ldots y_N$ are independent, then \n",
    "\n",
    "\\begin{equation}\n",
    "\tf(\\mathcal{D} \\vert w) = f(y_1 \\ldots y_N \\vert w) = \\prod_{k=1}^N f(y_k \\vert w)\n",
    "\t=\n",
    "\t\\prod_{k=1}^N  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y_k- x^Tw)^2)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Deriving the MAP criterion\n",
    "\n",
    "\\begin{exercise}\n",
    "Show the following relations\n",
    "\\end{exercise}\n",
    "\n",
    "**Additional Model Assumption**: \\$P(\\mathbf{w}) =\n",
    "\\frac{1}{\\sqrt{2\\pi\\tau\\^2}}e\\^{-\\frac{\\mathbf{w}\\^\\top\\mathbf{w}}{2\\tau\\^2}}\\$\\\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned} \\mathbf{w} &=\n",
    "\\underset {\\mathbf{w}} {argmax}\n",
    "P(\\mathbf{w}|y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n)\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax}\n",
    "\\frac{P(y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n|\\mathbf{w})P(\\mathbf{w})}{P(y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n)}\n",
    "\\\\\n",
    "&= \\underset {\\mathbf{w}} {argmax}\n",
    "P(y_1,\\mathbf{x}_1,...,y_n,\\mathbf{x}_n|\\mathbf{w})P(\\mathbf{w})\n",
    "\\\\\n",
    "&= \\underset {\\mathbf{w}} {argmax}\n",
    "\\left[\\prod_{i=1}^n P(y_i,\\mathbf{x}_i|\\mathbf{w})\\right]P(\\mathbf{w})\n",
    "\\\\\n",
    "&= \\underset {\\mathbf{w}} {argmax}\n",
    "\\left[\\prod_{i=1}^nP(y_i|\\mathbf{x}_i,\\mathbf{w})P(\\mathbf{x}_i|\\mathbf{w})\\right]P(\\mathbf{w})\n",
    "\\\\\n",
    "&= \\underset {\\mathbf{w}} {argmax}\n",
    "\\left[\\prod_{i=1}^nP(y_i|\\mathbf{x}_i,\\mathbf{w})P(\\mathbf{x}_i)\\right]P(\\mathbf{w})\n",
    "\\\\\n",
    "&= \\underset {\\mathbf{w}} {argmax}\n",
    "\\left[\\prod_{i=1}^n\n",
    "P(y_i|\\mathbf{x}_i,\\mathbf{w})\\right]P(\\mathbf{w})\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax} \\sum_{i=1}^n \n",
    "\\log P(y_i|\\mathbf{x}_i,\\mathbf{w})+ \\log P(\\mathbf{w})\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmax} -\\left(\\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^n (\\mathbf{x}_i^T\\mathbf{w}-y_i)^2 +\n",
    "\\frac{1}{2\\tau^2}\\mathbf{w}^T\\mathbf{w}\\right)\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmin} \\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^n (\\mathbf{x}_i^T\\mathbf{w}-y_i)^2 +\n",
    "\\frac{1}{2\\tau^2}\\mathbf{w}^T\\mathbf{w}\n",
    "\\\\ &=\n",
    "\\underset {\\mathbf{w}} {argmin} \\frac{1}{n}\n",
    "\\sum_{i=1}^n (\\mathbf{x}_i^T\\mathbf{w}-y_i)^2 + \\lambda||\n",
    "\\mathbf{w}||_2^2 & \n",
    "{\\lambda=\\frac{\\sigma^2}{n\\tau^2}} \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This objective is known as Ridge Regression. It has a closed form\n",
    "solution of: $\\mathbf{w} = (\\mathbf{X X}^T+\\lambda\n",
    "\\mathbf{I})^{-1}\\mathbf{X}\\mathbf{y}^T$ where\n",
    "$\\mathbf{X}=[\\mathbf{x}_1,\\dots,\\mathbf{x}_n]$\n",
    "and $\\mathbf{y}=[y_1,\\dots,y_n]$.\n",
    "\n",
    "**Relation between MAP and MLE**\n",
    "If $w$ is not random, then Maximum Likelihood is MAP, why ? because $P(w)$ is 1, i.e. a distribution of a non-random term. \n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Ordinary Least Squares:**\n",
    "\n",
    "-   \\$\\underset {\\mathbf{w}}{min}\n",
    "    \\frac{1}{n}\\sum_{i=1}^n\n",
    "    (\\mathbf{x}_i^\\top\\mathbf{w}-y_i)^2$.\n",
    "-   Squared loss.\n",
    "-   No regularization.\n",
    "-   Closed form: \\$\\mathbf{w} = (\\mathbf{X X\\^\\top})\\^{-1}\\mathbf{X}\n",
    "    \\mathbf{y}\\^\\top\\$.\n",
    "\n",
    "\n",
    "**Ridge Regression:**\n",
    "\n",
    "-   \\$\\operatorname\\*{min}\\_{\\mathbf{\\mathbf{w}}}\n",
    "    \\frac{1}{n}\\sum\\_{i=1}\\^n\n",
    "    (\\mathbf{x}\\_i\\^\\top\\mathbf{w}-y\\_i)\\^2 + \\lambda\n",
    "    ||\\mathbf{w}||\\_2\\^2\\$.\n",
    "-   Squared loss.\n",
    "-   \\$l2\\text{-regularization}\\$.\n",
    "-   Closed form: \\$\\mathbf{w} = (\\mathbf{X X\\^{\\top}}+\\lambda\n",
    "    \\mathbf{I})\\^{-1}\\mathbf{X} \\mathbf{y}\\^\\top\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
